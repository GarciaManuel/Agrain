# -*- coding: utf-8 -*-
"""Avocado_volume_predictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/175Afr6Lc77Un2WfvLhh3V0p8fUvmFxSe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import matplotlib.pyplot as plt  

def hypothesis(params, sample):
	cumulative = 0
	for i in range(len(params)):
		cumulative = cumulative + params[i]*sample[i]  
	return cumulative;

def gradient_descent(params, samples, y, alpha):
	temp = list(params)
	for j in range(len(params)):
		cumulative =0; 
		for i in range(len(samples)):
			error = hypothesis(params,samples[i]) - y[i]
			cumulative = cumulative + error*samples[i][j]  
		temp[j] = params[j] - alpha*(1/len(samples))*cumulative  
	return temp

def scale_query(avgs, maxs, query):
	temp = query
	for i in range(len(query)):
			temp[i] = (query[i] - avgs[i])/maxs[i]
	return temp

def scaling(samples):
	cumulative = 0
	samples = np.asarray(samples).T.tolist() #Check columns 
	avgs = [0]
	maxs = [1]

	for i in range(1,len(samples)):	
		for j in range(len(samples[i])):
			cumulative=+ samples[i][j]
		avg = cumulative/(len(samples[i]))
		max_val = max(samples[i])
		avgs.append(avg)
		maxs.append(max_val)
		for j in range(len(samples[i])):
			samples[i][j] = (samples[i][j] - avg)/max_val  #Mean scaling

	return avgs, maxs, np.asarray(samples).T.tolist()

def show_errors(params, samples,y):
	global __errors__
	error_acum =0

	for i in range(len(samples)):
		predicted_y = hypothesis(params,samples[i])
		error = predicted_y - y[i]
		error_acum=+error**2

	mean_error=error_acum/len(samples)
	__errors__.append(mean_error)
 
def get_params(sample):
  params = []
  for i in range(len(sample.columns)):
    params.append(0)
  return params

__errors__= []


os.chdir(os.path.dirname(__file__))
df = pd.read_csv('avocado.csv', index_col=0, encoding='latin-1')
train = df.loc[df['region'] == 'NewYork']

to_drop =['Total Volume','4046', '4770','Total Bags','Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region']
train.drop(columns=to_drop, axis=1, inplace=True)
train = train.dropna()
train = train.reset_index(drop = True) 
train['Date'] = train['Date'].str.replace("-","").astype(int)

y = train['4225'].values

train.drop(columns=['4225'], axis=1, inplace=True)
train.insert(0, 'Tetha', 1)

samples = train.values
params = get_params(train)


plt.scatter(train['AveragePrice'].values, y)
plt.show()
plt.scatter(train['Date'].values, y)
plt.show()
avgs, maxs, samples = scaling(samples)

epochs = 0
alfa =.23 

while True:
  oldparams = list(params)
  params=gradient_descent(params, samples,y,alfa)
  show_errors(params, samples, y)
  epochs = epochs + 1
  if(oldparams == params or epochs == 4000):  
    print ("final params:")
    print (params)
    break
  
plt.plot(__errors__)
plt.show()
print(__errors__)
print(f'Last mean error: {__errors__[-1]}')

str_date = input('Lets predict the amount of avocados to be sold, input the date of selling avocados (format Year-Month-Day Ex. 2015-04-26) ')
price =float(input('Now the price of the avocadoin US Dollars (point separated Ex. 1.2): '))

date = int(str_date.replace("-",""))
query = [1, date, price]
scalled_query =scale_query(avgs, maxs, query)
result = hypothesis(params, scalled_query)

print(f'The predicted volume is: {result} for an avocado price of {price} at the date {str_date}')